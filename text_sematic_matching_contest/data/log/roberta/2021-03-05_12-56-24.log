INFO: config {'data_dir': 'D:\\NLP-program\\text_sematic_matching_contest\\data\\Preliminary\\gaiic_track3_round1_train_20210228.tsv', 'task': 'TianChi', 'embed_dir': 'D:\\NLP-program\\text_sematic_matching_contest\\data\\vector', 'models_name': 'roberta', 'device': device(type='cpu'), 'requires_grad': True, 'class_list': [], 'num_labels': 2, 'train_num_examples': 0, 'dev_num_examples': 0, 'test_num_examples': 0, 'hidden_dropout_prob': 0.1, 'hidden_size': [512], 'early_stop': False, 'require_improvement': 500, 'num_train_epochs': 8, 'batch_size': 128, 'learning_rate': 2e-05, 'head_learning_rate': 0.001, 'weight_decay': 0.01, 'warmup_proportion': 0.1, 'k_fold': 5, 'multi_drop': 5, 'is_logging2file': True, 'logging_dir': 'D:\\NLP-program\\text_sematic_matching_contest\\data\\log\\roberta', 'load_save_model': False, 'save_path': ['D:\\NLP-program\\text_sematic_matching_contest\\data\\model_data'], 'save_file': ['roberta'], 'seed': 12345, 'loss_method': 'binary', 'diff_learning_rate': False, 'pattern': 'full_train', 'stop_word_valid': True, 'out_prob': True, 'n_gpu': 0, 'vocab_size': None, 'data_enhance': True, 'embeding_size': 128}
INFO: ***** Running training *****
INFO:   Train Num examples = 0
INFO:   Dev Num examples = 0
INFO:   Num Epochs = 8
INFO:   Instantaneous batch size GPU/CPU = 128
INFO:   Total optimization steps = 11880
INFO:   Train device:cpu
INFO: Epoch [1/8]
